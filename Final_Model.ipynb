{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score, KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "import tensorflow as tf\n",
        "\n",
        "# Data Loading and Preparation\n",
        "train_df = pd.read_csv('/content/transformed_train_2.csv')\n",
        "test_df = pd.read_csv('/content/transformed_test_2.csv')\n",
        "train_df = train_df.drop(columns=['DATE'])\n",
        "\n",
        "# Feature Selection\n",
        "features = [col for col in train_df.columns if col != 'TAVG' and col != 'Unnamed: 0']\n",
        "X_train = train_df[features]\n",
        "y_train = train_df['TAVG']\n",
        "X_test = test_df[features]\n",
        "\n",
        "# Feature Importance Analysis using ExtraTreesRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "extra_trees = ExtraTreesRegressor(random_state=42)\n",
        "extra_trees.fit(X_train, y_train)\n",
        "feature_importances = extra_trees.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "print(\"Feature Importance:\")\n",
        "print(importance_df)\n",
        "\n",
        "# Feature Reduction\n",
        "top_features = importance_df.head(20)['Feature'].tolist()\n",
        "X_train_reduced = X_train[top_features]\n",
        "X_test_reduced = X_test[top_features]\n",
        "\n",
        "corr_matrix = X_train_reduced.corr().abs()\n",
        "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
        "print(f\"Features to drop due to high correlation: {to_drop}\")\n",
        "X_train_reduced = X_train_reduced.drop(columns=to_drop)\n",
        "X_test_reduced = X_test_reduced.drop(columns=to_drop)\n",
        "\n",
        "# Recursive Feature Elimination (RFE)\n",
        "model = LinearRegression()\n",
        "rfe = RFE(model, n_features_to_select=10)\n",
        "fit = rfe.fit(X_train_reduced, y_train)\n",
        "selected_features_rfe = X_train_reduced.columns[fit.support_]\n",
        "print(f\"Selected features using RFE: {selected_features_rfe}\")\n",
        "\n",
        "X_train_reduced = X_train_reduced[selected_features_rfe]\n",
        "X_test_reduced = X_test_reduced[selected_features_rfe]\n",
        "\n",
        "# Define NAG optimizer using TensorFlow\n",
        "class NAGOptimizer(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9, name=\"NAGOptimizer\", **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._learning_rate = learning_rate\n",
        "        self._momentum = momentum\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "        momentum_var = self.add_weight(name=\"momentum\", shape=var.shape, initializer=\"zeros\", trainable=False)\n",
        "        momentum = self._momentum * momentum_var + grad\n",
        "        var.assign_sub(self._learning_rate * (grad + self._momentum * momentum))\n",
        "        momentum_var.assign(momentum)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"learning_rate\": self._learning_rate, \"momentum\": self._momentum})\n",
        "        return config\n",
        "\n",
        "# Hyperparameter Tuning with RandomizedSearchCV and GridSearchCV\n",
        "# RandomForestRegressor\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    param_distributions=param_grid_rf,\n",
        "    n_iter=10, cv=3, random_state=42\n",
        ")\n",
        "rf_random_search.fit(X_train_reduced, y_train)\n",
        "best_rf_params = rf_random_search.best_params_\n",
        "\n",
        "rf_grid_search = GridSearchCV(\n",
        "    RandomForestRegressor(**best_rf_params, random_state=42),\n",
        "    param_grid={'bootstrap': [True, False]},\n",
        "    cv=3\n",
        ")\n",
        "rf_grid_search.fit(X_train_reduced, y_train)\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "\n",
        "# GradientBoostingRegressor\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "gb_random_search = RandomizedSearchCV(\n",
        "    GradientBoostingRegressor(random_state=42),\n",
        "    param_distributions=param_grid_gb,\n",
        "    n_iter=10, cv=3, random_state=42\n",
        ")\n",
        "gb_random_search.fit(X_train_reduced, y_train)\n",
        "best_gb_params = gb_random_search.best_params_\n",
        "\n",
        "gb_grid_search = GridSearchCV(\n",
        "    GradientBoostingRegressor(**best_gb_params, random_state=42),\n",
        "    param_grid={'subsample': [0.8, 1.0]},\n",
        "    cv=3\n",
        ")\n",
        "gb_grid_search.fit(X_train_reduced, y_train)\n",
        "best_gb_model = gb_grid_search.best_estimator_\n",
        "\n",
        "# SGDRegressor with ADAM-like optimization\n",
        "adam_sgd = SGDRegressor(\n",
        "    max_iter=1000,\n",
        "    tol=1e-3,\n",
        "    learning_rate='constant',\n",
        "    eta0=0.01,  # Learning rate\n",
        "    penalty='elasticnet',  # ADAM-like approach using elasticnet\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "param_grid_sgd = {\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'l1_ratio': [0.15, 0.3, 0.5],\n",
        "    'learning_rate': ['constant', 'optimal', 'invscaling']\n",
        "}\n",
        "\n",
        "sgd_random_search = RandomizedSearchCV(\n",
        "    adam_sgd,\n",
        "    param_distributions=param_grid_sgd,\n",
        "    n_iter=10, cv=3, random_state=42\n",
        ")\n",
        "sgd_random_search.fit(X_train_reduced, y_train)\n",
        "best_sgd_params = sgd_random_search.best_params_\n",
        "\n",
        "sgd_grid_search = GridSearchCV(\n",
        "    adam_sgd.set_params(**best_sgd_params),\n",
        "    param_grid={'eta0': [0.01, 0.05, 0.1]},\n",
        "    cv=3\n",
        ")\n",
        "sgd_grid_search.fit(X_train_reduced, y_train)\n",
        "best_sgd_model = sgd_grid_search.best_estimator_\n",
        "\n",
        "# Model Initialization and Stacking\n",
        "base_models = [\n",
        "    ('rf', best_rf_model),\n",
        "    ('gb', best_gb_model),\n",
        "    ('sgd', best_sgd_model)  # SGD replaces ExtraTrees in the base models\n",
        "]\n",
        "\n",
        "stacking_regressor = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=best_sgd_model  # Using the SGD model with ADAM-like optimization\n",
        ")\n",
        "\n",
        "# Stacking Model Hyperparameter Tuning\n",
        "stacking_param_grid = {\n",
        "    'final_estimator__alpha': [0.0001, 0.001, 0.01],\n",
        "    'final_estimator__l1_ratio': [0.15, 0.3, 0.5]\n",
        "}\n",
        "\n",
        "stacking_grid_search = GridSearchCV(\n",
        "    stacking_regressor,\n",
        "    param_grid=stacking_param_grid,\n",
        "    cv=3\n",
        ")\n",
        "stacking_grid_search.fit(X_train_reduced, y_train)\n",
        "best_stacking_model = stacking_grid_search.best_estimator_\n",
        "\n",
        "# Pipeline Creation\n",
        "stacking_pipeline = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('stacking', best_stacking_model)\n",
        "])\n",
        "\n",
        "# Model Training and Evaluation\n",
        "stacking_pipeline.fit(X_train_reduced, y_train)\n",
        "y_train_pred = stacking_pipeline.predict(X_train_reduced)\n",
        "mae = mean_absolute_error(y_train, y_train_pred)\n",
        "mse = mean_squared_error(y_train, y_train_pred)\n",
        "print(f\"Stacking Regressor - Mean Absolute Error on Training Set: {mae}\")\n",
        "print(f\"Stacking Regressor - Mean Squared Error on Training Set: {mse}\")\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(stacking_pipeline, X_train_reduced, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
        "print(f\"Stacking Regressor - Cross-validated MAE: {-np.mean(cv_scores)}\\n\")\n",
        "\n",
        "# Making Predictions and Saving Results\n",
        "test_predictions = stacking_pipeline.predict(X_test_reduced)\n",
        "submission_df = pd.DataFrame({'INDEX': test_df.index, 'TAVG': test_predictions})\n",
        "submission_df.to_csv('/content/submission.csv', index=False)\n",
        "print(\"Model training and prediction completed. Results saved to 'submission.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSowWGRsvGTz",
        "outputId": "44af19e0-e315-4be1-e26c-535d713cf63d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance:\n",
            "                Feature  Importance\n",
            "32              DAY_COS    0.254134\n",
            "64          MEDIAN_TEMP    0.146496\n",
            "62            MEAN_TEMP    0.100877\n",
            "51       TEMP_ANOMALY_B    0.050472\n",
            "7                TAVG_A    0.048787\n",
            "..                  ...         ...\n",
            "38         ELEV_DIFF_AB    0.000000\n",
            "39         ELEV_DIFF_AC    0.000000\n",
            "1           LONGITUDE_A    0.000000\n",
            "53  LAT_LONG_INTERACT_A    0.000000\n",
            "80     LOCATION_CLUSTER    0.000000\n",
            "\n",
            "[81 rows x 2 columns]\n",
            "Features to drop due to high correlation: ['MEAN_TEMP', 'ELEV_TEMP_INTERACT_A', 'AVG_TEMP_B', 'MONTH_COS', 'TAVG_C', 'DAY_SIN', 'TMAX_B', 'MONTH', 'TEMP_ANOMALY_A']\n",
            "Selected features using RFE: Index(['DAY_COS', 'MEDIAN_TEMP', 'TEMP_ANOMALY_B', 'TAVG_A', 'TMIN_B',\n",
            "       'MONTH_SIN', 'DAY_OF_YEAR', 'TMAX_A', 'TMAX_C', 'TMIN_C'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "15 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.97546258        nan 0.9751833         nan\n",
            " 0.9758545  0.974368          nan 0.97469746]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Regressor - Mean Absolute Error on Training Set: 0.9326088260388901\n",
            "Stacking Regressor - Mean Squared Error on Training Set: 1.2821688484146592\n",
            "Stacking Regressor - Cross-validated MAE: 1.8251744386986501\n",
            "\n",
            "Model training and prediction completed. Results saved to 'submission.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LN1tZqBrvGWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_o-zWc5QuoLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}