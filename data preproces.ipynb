{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnI2jsAPrXImcBe+Av5qTK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"dZGld4AhfKHQ","executionInfo":{"status":"error","timestamp":1725200682505,"user_tz":-330,"elapsed":7307,"user":{"displayName":"Naveen kumar Subramanian","userId":"17015125204102779347"}},"outputId":"25f51e5d-8015-4867-e579-2236acd5e8fb"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2f23a1fe7e20>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"]}],"source":["import pandas as pd\n","from sklearn.impute import KNNImputer\n","from sklearn.experimental import enable_iterative_imputer  # noqa\n","from sklearn.impute import IterativeImputer\n","import numpy as np\n","from datetime import datetime\n","from geopy.distance import geodesic\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n","from sklearn.linear_model import BayesianRidge\n","\n","# Load the datasets\n","train_df = pd.read_csv('/content/train.csv')\n","test_df = pd.read_csv('/content/test.csv')\n","\n","# Function to engineer features from the DATE column\n","def create_date_features(df):\n","    # Adjust the date format to match the actual format in your dataset\n","    df['DATE'] = pd.to_datetime(df['DATE'], format='%d-%m-%Y')  # Use dayfirst=True if date format varies\n","    df['DAY_OF_WEEK'] = df['DATE'].dt.dayofweek\n","    df['MONTH'] = df['DATE'].dt.month\n","    df['DAY_OF_YEAR'] = df['DATE'].dt.dayofyear\n","    df['YEAR'] = df['DATE'].dt.year\n","    df['IS_WEEKEND'] = df['DAY_OF_WEEK'].apply(lambda x: 1 if x >= 5 else 0)\n","    df['DAY_SIN'] = np.sin(2 * np.pi * df['DAY_OF_YEAR'] / 365.25)\n","    df['DAY_COS'] = np.cos(2 * np.pi * df['DAY_OF_YEAR'] / 365.25)\n","    df['MONTH_SIN'] = np.sin(2 * np.pi * df['MONTH'] / 12)\n","    df['MONTH_COS'] = np.cos(2 * np.pi * df['MONTH'] / 12)\n","    return df\n","\n","\n","# Function to calculate distance between locations\n","def calculate_distances(df):\n","    df['DISTANCE_AB'] = df.apply(lambda x: geodesic((x['LATITUDE_A'], x['LONGITUDE_A']),\n","                                                     (x['LATITUDE_B'], x['LONGITUDE_B'])).km, axis=1)\n","    df['DISTANCE_AC'] = df.apply(lambda x: geodesic((x['LATITUDE_A'], x['LONGITUDE_A']),\n","                                                     (x['LATITUDE_C'], x['LONGITUDE_C'])).km, axis=1)\n","    df['DISTANCE_BC'] = df.apply(lambda x: geodesic((x['LATITUDE_B'], x['LONGITUDE_B']),\n","                                                     (x['LATITUDE_C'], x['LONGITUDE_C'])).km, axis=1)\n","    df['ELEV_DIFF_AB'] = df['ELEVATION_A'] - df['ELEVATION_B']\n","    df['ELEV_DIFF_AC'] = df['ELEVATION_A'] - df['ELEVATION_C']\n","    df['ELEV_DIFF_BC'] = df['ELEVATION_B'] - df['ELEVATION_C']\n","    return df\n","\n","# Function to calculate weather-based features\n","def calculate_weather_features(df):\n","    df['TEMP_DIFF_A'] = df['TMAX_A'] - df['TMIN_A']\n","    df['TEMP_DIFF_B'] = df['TMAX_B'] - df['TMIN_B']\n","    df['TEMP_DIFF_C'] = df['TMAX_C'] - df['TMIN_C']\n","\n","    df['AVG_TEMP_A'] = df[['TMAX_A', 'TMIN_A']].mean(axis=1)\n","    df['AVG_TEMP_B'] = df[['TMAX_B', 'TMIN_B']].mean(axis=1)\n","    df['AVG_TEMP_C'] = df[['TMAX_C', 'TMIN_C']].mean(axis=1)\n","\n","    df['CUMULATIVE_PRCP'] = df[['PRCP_A', 'PRCP_B', 'PRCP_C']].sum(axis=1)\n","    df['CUMULATIVE_SNWD'] = df[['SNWD_A', 'SNWD_B', 'SNWD_C']].sum(axis=1)\n","\n","    df['TEMP_RANGE'] = df[['TMAX_A', 'TMAX_B', 'TMAX_C']].max(axis=1) - df[['TMIN_A', 'TMIN_B', 'TMIN_C']].min(axis=1)\n","\n","    # Calculate anomalies\n","    long_term_avg_temp = df[['TAVG_A', 'TAVG_B', 'TAVG_C']].mean().mean()\n","    df['TEMP_ANOMALY_A'] = df['AVG_TEMP_A'] - long_term_avg_temp\n","    df['TEMP_ANOMALY_B'] = df['AVG_TEMP_B'] - long_term_avg_temp\n","    df['TEMP_ANOMALY_C'] = df['AVG_TEMP_C'] - long_term_avg_temp\n","\n","    return df\n","\n","# Function to create interaction features\n","def create_interaction_features(df):\n","    df['LAT_LONG_INTERACT_A'] = df['LATITUDE_A'] * df['LONGITUDE_A']\n","    df['LAT_LONG_INTERACT_B'] = df['LATITUDE_B'] * df['LONGITUDE_B']\n","    df['LAT_LONG_INTERACT_C'] = df['LATITUDE_C'] * df['LONGITUDE_C']\n","\n","    df['ELEV_TEMP_INTERACT_A'] = df['ELEVATION_A'] * df['TAVG_A']\n","    df['ELEV_TEMP_INTERACT_B'] = df['ELEVATION_B'] * df['TAVG_B']\n","    df['ELEV_TEMP_INTERACT_C'] = df['ELEVATION_C'] * df['TAVG_C']\n","\n","    df['PRCP_INTERACT_AB'] = df['PRCP_A'] * df['PRCP_B']\n","    df['PRCP_INTERACT_AC'] = df['PRCP_A'] * df['PRCP_C']\n","    df['PRCP_INTERACT_BC'] = df['PRCP_B'] * df['PRCP_C']\n","\n","    return df\n","\n","# Function to create statistical summary features\n","def create_statistical_features(df):\n","    df['MEAN_TEMP'] = df[['TAVG_A', 'TAVG_B', 'TAVG_C']].mean(axis=1)\n","    df['VAR_TEMP'] = df[['TAVG_A', 'TAVG_B', 'TAVG_C']].var(axis=1)\n","    df['MEDIAN_TEMP'] = df[['TAVG_A', 'TAVG_B', 'TAVG_C']].median(axis=1)\n","\n","    df['MEAN_PRCP'] = df[['PRCP_A', 'PRCP_B', 'PRCP_C']].mean(axis=1)\n","    df['VAR_PRCP'] = df[['PRCP_A', 'PRCP_B', 'PRCP_C']].var(axis=1)\n","    df['MEDIAN_PRCP'] = df[['PRCP_A', 'PRCP_B', 'PRCP_C']].median(axis=1)\n","\n","    return df\n","\n","# Function to create lagged features\n","def create_lagged_features(df):\n","    df['TAVG_A_LAG1'] = df['TAVG_A'].shift(1)\n","    df['TAVG_B_LAG1'] = df['TAVG_B'].shift(1)\n","    df['TAVG_C_LAG1'] = df['TAVG_C'].shift(1)\n","\n","    df['PRCP_A_LAG1'] = df['PRCP_A'].shift(1)\n","    df['PRCP_B_LAG1'] = df['PRCP_B'].shift(1)\n","    df['PRCP_C_LAG1'] = df['PRCP_C'].shift(1)\n","\n","    df['ROLLING_MEAN_TEMP_A'] = df['TAVG_A'].rolling(window=3).mean()\n","    df['ROLLING_MEAN_TEMP_B'] = df['TAVG_B'].rolling(window=3).mean()\n","    df['ROLLING_MEAN_TEMP_C'] = df['TAVG_C'].rolling(window=3).mean()\n","\n","    df['ROLLING_MEAN_PRCP_A'] = df['PRCP_A'].rolling(window=3).mean()\n","    df['ROLLING_MEAN_PRCP_B'] = df['PRCP_B'].rolling(window=3).mean()\n","    df['ROLLING_MEAN_PRCP_C'] = df['PRCP_C'].rolling(window=3).mean()\n","\n","    return df\n","\n","# Function to create clusters based on geographical data\n","def create_location_clusters(df, n_clusters=3):\n","    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n","    df['LOCATION_CLUSTER'] = kmeans.fit_predict(df[['LATITUDE_A', 'LONGITUDE_A', 'ELEVATION_A']])\n","    return df\n","\n","# Define the ensemble of estimators\n","estimators = [\n","    RandomForestRegressor(n_estimators=10, random_state=0),\n","    GradientBoostingRegressor(n_estimators=10, random_state=0),\n","    ExtraTreesRegressor(n_estimators=10, random_state=0),\n","    BayesianRidge()\n","]\n","\n","# Ensemble Iterative Imputer Function\n","def ensemble_iterative_imputer(X, estimators, n_iter=10):\n","    imputations = []\n","    for estimator in estimators:\n","        imputer = IterativeImputer(estimator=estimator, max_iter=n_iter, random_state=0)\n","        imputed_data = imputer.fit_transform(X)\n","        imputations.append(imputed_data)\n","\n","    # Average the imputations\n","    averaged_imputations = np.mean(imputations, axis=0)\n","    return pd.DataFrame(averaged_imputations, columns=X.columns)\n","\n","# Function to impute missing values, excluding non-numeric columns\n","def impute_missing_values(df):\n","    # Separate numeric and non-numeric data\n","    numeric_df = df.select_dtypes(include=[np.number])\n","    non_numeric_df = df.select_dtypes(exclude=[np.number])\n","\n","    # Impute missing values in numeric data using Ensemble Iterative Imputer\n","    imputed_numeric_df = ensemble_iterative_imputer(numeric_df, estimators)\n","\n","    # Concatenate the imputed numeric data with non-numeric data\n","    df = pd.concat([imputed_numeric_df, non_numeric_df], axis=1)\n","\n","    return df\n","\n","\n","\n","# Engineer the features\n","train_df = create_date_features(train_df)\n","test_df = create_date_features(test_df)\n","\n","train_df = calculate_distances(train_df)\n","test_df = calculate_distances(test_df)\n","\n","train_df = calculate_weather_features(train_df)\n","test_df = calculate_weather_features(test_df)\n","\n","train_df = create_interaction_features(train_df)\n","test_df = create_interaction_features(test_df)\n","\n","train_df = create_statistical_features(train_df)\n","test_df = create_statistical_features(test_df)\n","\n","train_df = create_lagged_features(train_df)\n","test_df = create_lagged_features(test_df)\n","\n","train_df = create_location_clusters(train_df)\n","test_df = create_location_clusters(test_df)\n","\n","# Impute missing values\n","train_df = impute_missing_values(train_df)\n","test_df = impute_missing_values(test_df)\n","\n","# Save the transformed datasets\n","train_df.to_csv('transformed_train.csv', index=False)\n","test_df.to_csv('transformed_test.csv', index=False)\n","\n","# Optionally, display the first few rows of the transformed data\n","print(train_df.head())\n","print(test_df.head())\n"]},{"cell_type":"code","source":[],"metadata":{"id":"byYY33QSfrMp"},"execution_count":null,"outputs":[]}]}